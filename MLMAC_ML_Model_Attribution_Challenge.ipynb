{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarhanDhanani/MLMAC/blob/main/MLMAC_ML_Model_Attribution_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> This Notebook is prepared to submit the solutions to the \n",
        "<a href=\"https://mlmac.io/#overview\">MLMAC CONTEST</a> from the FAST-MT Team.</center></h1>\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/e/e4/National_University_of_Computer_and_Emerging_Sciences_logo.png\"/></center>\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XgZnwD58BhtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple set of booleans that represents the notebook is running on google colab or locally. And the model are already downloaded and loaded either on G-Drive or locally or it needs to be downloaded from the checkpoint repository."
      ],
      "metadata": {
        "id": "PbM4baAZB18N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_2qPJnmJHCo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "## A set of simple booleeans:\n",
        "##  - (run_on_colab) = True : if notebook is running on colab\n",
        "##  - (run_on_colab) = False : if notebook is running locally\n",
        "'''\n",
        "run_on_colab = True\n",
        "'''\n",
        "##  - (call_finetune_API) = True : allow to call finetune model API\n",
        "##  - (call_finetune_API) = False : do not allow to call finetune model API\n",
        "'''\n",
        "call_finetune_API = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing required dependencies and setuping base paths"
      ],
      "metadata": {
        "id": "Bml-yElNCZRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzAoe9ElIVmH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "##  - Mounting google drive to colab\n",
        "##  - Setting up base paths for accessing google drive\n",
        "'''\n",
        "if run_on_colab:\n",
        "    from google.colab import drive\n",
        "    from google.colab import files\n",
        "    base_path = '/content/drive'\n",
        "    drive.mount(base_path)\n",
        "    base_path = base_path + '/My Drive/'\n",
        "\n",
        "'''\n",
        "##  - Setting up base paths for accessing local drive\n",
        "'''\n",
        "else:\n",
        "    base_path = '/Users/fdhanani/Desktop/'\n",
        "\n",
        "base_path = base_path+\"/dataset/MLMAC/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJTA1wk5Bgh4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "##  - Installing required Dependecies\n",
        "'''\n",
        "!pip install apache_beam mwparserfromhell datasets\n",
        "!pip install transformers sentencepiece\n",
        "!pip install pyter3 sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpOZp9o1JPk9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import pyter\n",
        "import string\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import seed\n",
        "from pprint import pprint\n",
        "from random import randint\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "from sklearn.model_selection import KFold\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers.pipelines.conversational import Conversation\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import create_optimizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "import glob\n",
        "import nltk\n",
        "\n",
        "nltk.download('popular')\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DIRECTORY STRUCTURE:** \n",
        "\n",
        "Before executing the subsequent notebook cells, please clone all the related files from the [official Github repository](https://github.com/FarhanDhanani/joker-clef-22-FAST-MT). And ensure the mentioned directory structure is set up correctly either on the Google Drive if you are executing on the collab or your local system, in case you are running it locally.\n",
        "\n",
        "**BASE PATH: */dataset/MLMAC/**\n",
        "\n",
        "\n",
        "```\n",
        ".\n",
        "├── ML Model Attribution Challenge.ipynb (MAIN SOURCE FILE)\n",
        "│── Readme.md\n",
        "├── dataset\n",
        "│   └── MLMAC\n",
        "│       ├── bleu_ter_scoring.json\n",
        "│       ├── queries.json\n",
        "│       ├── base_models\n",
        "│       │   ├── bloom-2b5_api.json\n",
        "│       │   ├── bloom-350m_api.json\n",
        "│       │   ├── gpt-j-6B_api.json\n",
        "│       │   ├── gpt2-xl_api.json\n",
        "│       │   ├── model_bloom-350m.json\n",
        "│       │   ├── model_codegen-350M-multi.json\n",
        "│       │   ├── model_DialoGPT-large.json\n",
        "│       │   ├── model_distilgpt2.json\n",
        "│       │   ├── model_gpt-neo-125M.json\n",
        "│       │   ├── model_gpt2.json\n",
        "│       │   ├── model_Multilingual-MiniLM-L12-H384.json\n",
        "│       │   ├── model_opt-350m.json\n",
        "│       │   ├── xlnet-base-cased_api.json\n",
        "│       ├── finetuned_models\n",
        "│       │   ├── 0_api.json\n",
        "│       │   ├── 1_api.json\n",
        "│       │   ├── 2_api.json\n",
        "│       │   ├── 3_api.json\n",
        "│       │   ├── 4_api.json\n",
        "│       │   ├── 5_api.json\n",
        "│       │   ├── 6_api.json\n",
        "│       │   ├── 7_api.json\n",
        "│       │   ├── 8_api.json\n",
        "│       │   ├── 9_api.json\n",
        "│       │   ├── 10_api.json\n",
        "│       │   ├── 11_api.json\n",
        "│       ├── FT\n",
        "│       │   ├── albert-base-v2/*\n",
        "│       │   ├── distilbert-base-uncased/*\n",
        "│       │   ├── roberta-base/*\n",
        "│       │   ├── other-saved-tokenizer-files\n",
        "│       │   ├── google\n",
        "│       │   │   ├──  electra-small-discriminator/*\n",
        "│       │   │\n",
        "│       │   .\n",
        "│       .\n",
        "├── .\n",
        "```"
      ],
      "metadata": {
        "id": "PcRpWB8b__ZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA SETS FOR ASSEMBLING QUERIES\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lzDKioByDhrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING GLUE DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "T2xl125MEYoR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0RFMBUQqSyO"
      },
      "source": [
        "\n",
        "Below, we have mentioned the correct citation for the GLUE data set.\n",
        "```\n",
        "@inproceedings{wang2019glue,\n",
        "  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
        "  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
        "  note={In the Proceedings of ICLR.},\n",
        "  year={2019}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2ob2VcmHWK3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "##  - Loading the GLUE dataset\n",
        "'''\n",
        "dataset1 = load_dataset(\"glue\", 'cola')\n",
        "dataset1['train'][0]['sentence']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING ROTTEN TOMATOES DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aOJhvnw6UOdv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ov2zVzPqCV1"
      },
      "source": [
        "Below, we have mentioned the correct citation for the ROTTEN TOMATOES data set.\n",
        "\n",
        "\n",
        "```\n",
        "@InProceedings{Pang+Lee:05a,\n",
        "  author =       {Bo Pang and Lillian Lee},\n",
        "  title =        {Seeing stars: Exploiting class relationships for sentiment\n",
        "                  categorization with respect to rating scales},\n",
        "  booktitle =    {Proceedings of the ACL},\n",
        "  year =         2005\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaZ5thh_IjV2"
      },
      "outputs": [],
      "source": [
        "dataset2 = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "dataset2[0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING SQUAD_V2 DATA SET\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "eMWVmITZU0pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we have mentioned the correct citation for the SQUAD_V2 data set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{2016arXiv160605250R,\n",
        "       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n",
        "                 Konstantin and {Liang}, Percy},\n",
        "        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n",
        "      journal = {arXiv e-prints},\n",
        "         year = 2016,\n",
        "          eid = {arXiv:1606.05250},\n",
        "        pages = {arXiv:1606.05250},\n",
        "archivePrefix = {arXiv},\n",
        "       eprint = {1606.05250},\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WDjiqqkAU6p8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uinsqQ1eJhvz"
      },
      "outputs": [],
      "source": [
        "dataset3 = load_dataset(\"squad_v2\", split='train')\n",
        "dataset3[0]['answers']['text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING IMDB DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "b3ecAvdMVakM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U29gDeJ8pnTK"
      },
      "source": [
        "Below, we have mentioned the correct citation for the IMDB data set.\n",
        "\n",
        "```\n",
        "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
        "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
        "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
        "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
        "  month     = {June},\n",
        "  year      = {2011},\n",
        "  address   = {Portland, Oregon, USA},\n",
        "  publisher = {Association for Computational Linguistics},\n",
        "  pages     = {142--150},\n",
        "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcet2OLTLmzN"
      },
      "outputs": [],
      "source": [
        "dataset4 = load_dataset(\"imdb\", split='test')\n",
        "dataset4[0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING TREC DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Aev_Ys22V6C1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEJtz8oCpdtb"
      },
      "source": [
        "Below, we have mentioned the correct citation for the TREC data set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@inproceedings{li-roth-2002-learning,\n",
        "    title = \"Learning Question Classifiers\",\n",
        "    author = \"Li, Xin  and\n",
        "      Roth, Dan\",\n",
        "    booktitle = \"{COLING} 2002: The 19th International Conference on Computational Linguistics\",\n",
        "    year = \"2002\",\n",
        "    url = \"https://www.aclweb.org/anthology/C02-1150\",\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S7jtf6wMMEH"
      },
      "outputs": [],
      "source": [
        "dataset5 = load_dataset(\"trec\", split=\"test\")\n",
        "dataset5[0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING WMT22 AFRICAN DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qBprJcYlWlJ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMSGFNqjYY2U"
      },
      "source": [
        "Below, we have mentioned the correct citation for the WMT22 AFRICAN data set.\n",
        "\n",
        "```\n",
        "@misc{https://doi.org/10.48550/arxiv.2207.04672,\n",
        "  doi = {10.48550/ARXIV.2207.04672}, \n",
        "  url = {https://arxiv.org/abs/2207.04672},\n",
        "  author = {{NLLB Team} and Costa-jussà, Marta R. and Cross, James and Çelebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzmán, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},\n",
        "  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7, 68T50},\n",
        "  title = {No Language Left Behind: Scaling Human-Centered Machine Translation},\n",
        "  publisher = {arXiv},\n",
        "  year = {2022},\n",
        "  copyright = {Creative Commons Attribution Share Alike 4.0 International}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTVfAgTGXJZN"
      },
      "outputs": [],
      "source": [
        "dataset6 = load_dataset(\"allenai/wmt22_african\", 'afr-eng', split='train')\n",
        "dataset6[0]['translation']['eng']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING OSCAR MINI DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MIPdiUC_XT2I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X3AZ9eui7hh"
      },
      "source": [
        "\n",
        "Below, we have mentioned the correct citation for the OSCAR MINI data set.\n",
        "```\n",
        "@inproceedings{ortiz-suarez-etal-2020-monolingual,\n",
        "    title = \"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages\",\n",
        "    author = \"Ortiz Su{'a}rez, Pedro Javier  and\n",
        "      Romary, Laurent  and\n",
        "      Sagot, Benoit\",\n",
        "    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n",
        "    month = jul,\n",
        "    year = \"2020\",\n",
        "    address = \"Online\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://www.aclweb.org/anthology/2020.acl-main.156\",\n",
        "    pages = \"1703--1714\",\n",
        "    abstract = \"We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.\",\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk13RGxxYxD4"
      },
      "outputs": [],
      "source": [
        "dataset7 = load_dataset(\"nthngdy/oscar-mini\", 'unshuffled_deduplicated_en', split=\"train\")\n",
        "dataset7[0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING RACE DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rz1O92KAYocD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3iDRLRfi5c-"
      },
      "source": [
        "\n",
        "Below, we have mentioned the correct citation for the RACE data set.\n",
        "```\n",
        "@article{lai2017large,\n",
        "    title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},\n",
        "    author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},\n",
        "    journal={arXiv preprint arXiv:1704.04683},\n",
        "    year={2017}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khnTdqVzZGeG"
      },
      "outputs": [],
      "source": [
        "dataset8 = load_dataset(\"race\", \"middle\", split=\"test\")\n",
        "dataset8[0][\"question\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING BLIMP DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IUD0MQz6ZF_i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUtZtSaoix87"
      },
      "source": [
        "Below, we have mentioned the correct citation for the BLIMP data set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{warstadt2019blimp,\n",
        "  title={BLiMP: A Benchmark of Linguistic Minimal Pairs for English},\n",
        "  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei, and Wang, Sheng-Fu and Bowman, Samuel R},\n",
        "  journal={arXiv preprint arXiv:1912.00582},\n",
        "  year={2019}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs_YD5y_cmeW"
      },
      "outputs": [],
      "source": [
        "dataset9 = load_dataset(\"blimp\", 'adjunct_island', split='train')\n",
        "dataset9[0]['sentence_good']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING  PIQA DATA SET\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DbO6-tI_ZVSF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX5Vo8Q8ijUg"
      },
      "source": [
        "Below, we have mentioned the correct citation for the PIQA data set.\n",
        "\n",
        "```\n",
        "@inproceedings{Bisk2020,\n",
        "  author = {Yonatan Bisk and Rowan Zellers and\n",
        "            Ronan Le Bras and Jianfeng Gao\n",
        "            and Yejin Choi},\n",
        "  title = {PIQA: Reasoning about Physical Commonsense in\n",
        "           Natural Language},\n",
        "  booktitle = {Thirty-Fourth AAAI Conference on\n",
        "               Artificial Intelligence},\n",
        "  year = {2020},\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxpvlxoIhRlS"
      },
      "outputs": [],
      "source": [
        "dataset10= load_dataset('piqa', split='test')\n",
        "dataset10[0]['goal']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASSEMBLING QUERIES FOR INTERROGATING MODELS\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3PrCjCi_kXUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will assemble a list of ninety distinct queries, which we can then use later use for interrogating pre-trained base models and fine-tuned models."
      ],
      "metadata": {
        "id": "TYNO8ooTmKps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEFINING FUNCTION FOR ASSEMBLING QUERIES\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MnX52svMoM81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czq7uIUDiY1b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## Function to get a list of queries\n",
        "\n",
        "  # Function takes no input\n",
        "  # Function assembles a list of ninety distinct queries\n",
        "  # In other words function assembles a list of hundred queries from above loaded\n",
        "    datasets with 10 duplicates\n",
        "'''\n",
        "\n",
        "def getQueries():\n",
        "\n",
        "  # Loading data sets\n",
        "  dataset1 = load_dataset(\"glue\", 'cola')\n",
        "  dataset2 = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "  dataset3 = load_dataset(\"squad_v2\", split='train')\n",
        "  dataset4 = load_dataset(\"imdb\", split='test')\n",
        "  dataset5 = load_dataset(\"trec\", split=\"test\")\n",
        "  dataset6 = load_dataset(\"allenai/wmt22_african\", 'afr-eng', split='train')\n",
        "  dataset7 = load_dataset(\"nthngdy/oscar-mini\", 'unshuffled_deduplicated_en', split=\"train\")\n",
        "  dataset8 = load_dataset(\"race\", \"middle\", split=\"test\")\n",
        "  dataset9 = load_dataset(\"blimp\", 'adjunct_island', split='train')\n",
        "  dataset10= load_dataset('piqa', split='test')\n",
        "  \n",
        "  # Assembeling queries randomly\n",
        "  seed(1)\n",
        "  queries=[]\n",
        "  len_threshold = 60\n",
        "  iterations = 10\n",
        "  for it in range(iterations):\n",
        "    index = randint(0, 500)\n",
        "    \n",
        "    dataset1_query = 'c'* len_threshold\n",
        "    while(len(dataset1_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset1_query = dataset1['train'][index]['sentence']\n",
        "      \n",
        "    dataset2_query = 'c'* len_threshold\n",
        "    while(len(dataset2_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset2_query = dataset2[index]['text']\n",
        "      \n",
        "    dataset3_query = 'c'* len_threshold\n",
        "    while(len(dataset3_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset3_query = dataset3[index]['answers']['text'][0]\n",
        "      \n",
        "    dataset4_query = 'c'* len_threshold\n",
        "    while(len(dataset4_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset4_query = dataset4[index]['text'] \n",
        "  \n",
        "    dataset5_query = 'c'* len_threshold\n",
        "    while(len(dataset5_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset5_query = dataset5[index]['text']\n",
        "      \n",
        "    dataset6_query = 'c'* len_threshold\n",
        "    while(len(dataset6_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset6_query = dataset6[index]['translation']['eng']\n",
        "    \n",
        "    if(it==iterations-1):\n",
        "      dataset7_query = 'c'\n",
        "      while(len(dataset7_query)<len_threshold):\n",
        "        index = randint(0, 5000)\n",
        "        dataset7_query = dataset6[index]['translation']['eng']\n",
        "    else:\n",
        "      index = randint(0, 5000)\n",
        "      dataset7_query = dataset7[index]['text'][0:len_threshold-1]\n",
        " \n",
        "    dataset8_query = 'c'* len_threshold\n",
        "    while(len(dataset8_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset8_query = dataset8[index][\"question\"]\n",
        "      \n",
        "    dataset9_query = 'c'* len_threshold\n",
        "    while(len(dataset9_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset9_query = dataset9[index]['sentence_good']\n",
        "  \n",
        "    dataset10_query = 'c'* len_threshold\n",
        "    while(len(dataset10_query)>=len_threshold):\n",
        "      index = randint(0, 500)\n",
        "      dataset10_query = dataset10[index]['goal']\n",
        "\n",
        "    queries.append(dataset1_query)\n",
        "    queries.append(dataset2_query)\n",
        "    queries.append(dataset3_query)\n",
        "    queries.append(dataset4_query)\n",
        "    queries.append(dataset5_query)\n",
        "    queries.append(dataset6_query)\n",
        "    queries.append(dataset8_query)\n",
        "    queries.append(dataset9_query)\n",
        "    queries.append(dataset10_query)\n",
        "    queries.append(dataset7_query)\n",
        "    \n",
        "  return queries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXECUTING THE FUNCTION TO ASSEMBLE QUERY & SAVING THE RETURNED RESULT\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eyZf71SdoePZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9IYsIWE_tU_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## Calling the getQueries function and saving the \n",
        "  ## returned response in the permanent file system.\n",
        "'''\n",
        "\n",
        "queries = getQueries() \n",
        "with open(base_path+\"queries.json\", \"w\") as outfile:\n",
        "    outfile.write(json.dumps(queries, indent = 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL DEFINATION\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yOG1FjMDpld7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will create functions and classes that can allow us to query the pre-trained base and fine-tuned models via REST APIs. Let's first define a model class that contains all the required details to accomplish this task."
      ],
      "metadata": {
        "id": "mPhHq7tOvtl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfU1n7W3DKpq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## Defining a MODEL class that we can use later to query \n",
        "  ## pre-trained base models and fine-tuned models via REST APIs\n",
        "'''\n",
        "\n",
        "class Model:\n",
        "\n",
        "  '''\n",
        "    ## The function initiallizes the instance of the Model class\n",
        "\n",
        "    ## The function takes four inputs\n",
        "    \n",
        "    ## The First input, \"API,\" entails which endpoint the instance \n",
        "       will use to fetch a response for the given query.\n",
        "\n",
        "       ## The first option is \"hf,\" which is the endpoint to query base models.\n",
        "       ## The second option is to use the other endpoint to query fine-tuned models.\n",
        "    \n",
        "    ## The second input is \"api_token\" which is required to authenticate the calls. \n",
        "       Plus, the token will keep track of the requesting caller and verify \n",
        "       the request for security purposes.\n",
        "    \n",
        "    ## The third input is \"model_id,\" which specifies the index of the model \n",
        "       that we want to query from the set of either base or fine-tuned models.\n",
        "\n",
        "    ## The last boolean \"us_cache\" variable specifies whether we want \n",
        "       to keep the queried request along with their returned \n",
        "       responses in memory to save duplicate REST calls.\n",
        "  '''\n",
        "  def __init__(self, api, api_token, model_id, use_cache=True):\n",
        "    self.api = api\n",
        "    self.api_token = api_token\n",
        "    self.model_id = model_id\n",
        "    self.use_cache = use_cache\n",
        "    self.cache = {}\n",
        "\n",
        "    if api == \"hf\":\n",
        "      if model_id == \"gpt-j-6B\":\n",
        "        self.api_url = f\"https://api-inference.huggingface.co/models/EleutherAI/gpt-j-6B\"\n",
        "      else:\n",
        "        self.api_url = f\"https://api-inference.huggingface.co/models/model-attribution-challenge/{model_id}\"\n",
        "    elif api == \"mlmac\":\n",
        "      self.api_url = f\"https://api.mlmac.io:8080/query?model={model_id}\"\n",
        "\n",
        "  \n",
        "\n",
        "  '''\n",
        "    ## The function executes the REST call to fetch response for a given query\n",
        "\n",
        "    ## The function takes three inputs\n",
        "    \n",
        "    ## The First input, \"input,\" conntains a query.\n",
        "    ## The Second input is \"max_retries\" which is determines the number of times\n",
        "       to retry if the API fails to deliver response timely.\n",
        "    ## The Third input is \"params,\" which specifies the additional parameters \n",
        "       required for the request.\n",
        "    ## The last input is \"options,\" which specifies other optional parameters \n",
        "       for executing the request.\n",
        "    \n",
        "    ## The function returns the result of the REST API call as its output \n",
        "    ## that contains the response for the given query.\n",
        "  '''\n",
        "  def __call__(self, input, max_retries=10, params={}, options={}):\n",
        "    \n",
        "    retry_duration = 20.0\n",
        "    \n",
        "    if self.use_cache and input in self.cache:\n",
        "      print(\"response is from cache\")\n",
        "      return self.cache[input]\n",
        "    \n",
        "    if self.api == \"hf\":\n",
        "      payload = {\"inputs\": input, \"parameters\": params, \"options\": options}\n",
        "    elif self.api == \"mlmac\":\n",
        "      payload = {\"input\": input}\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
        "\n",
        "    for retry in range(max_retries):\n",
        "      response = requests.post(self.api_url, json=payload, headers=headers)\n",
        "    \n",
        "      if response.status_code == 200:\n",
        "        if self.api == \"hf\":\n",
        "          result = response.json()\n",
        "        elif self.api == \"mlmac\":\n",
        "          result = response.json().get(\"result\")\n",
        "\n",
        "        self.cache[input] = result\n",
        "\n",
        "        return result\n",
        "      elif response.status_code == 503:\n",
        "        print(response.json())\n",
        "        print(f\"attempt {retry+1}/{max_retries}; waiting for 20 seconds\")\n",
        "        time.sleep(retry_duration)\n",
        "      else: # error\n",
        "        raise Exception(response.text)\n",
        "    \n",
        "    raise Exception(f\"Failed after {max_retries} attempts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COLLECTING RESPOSES FROM THE BASE MODELS\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "X1DnSmkE2fsa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7jb-8mjzlrQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## The function \"getResponsesFromBaseModel\" uses the provided array of \n",
        "     base models to return the responses from the base model specified by \n",
        "     the \"model_index\" for the given queries.\n",
        "\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## base_models: is the list of all the base models provided by the MLMAC\n",
        "  ## base_model_names: is the names of all the base models provided by the MLMAC\n",
        "  ## queries: contains list of all assembled queries\n",
        "  ## model_index: is the index of the model whose responses are needed to get record\n",
        "\n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function will return the list of responses generated by the specified\n",
        "     base model for the given list of queries\n",
        "'''\n",
        "def getResponsesFromBaseModel(model_index, base_model_names, base_models, queries):\n",
        "  model_responses = []\n",
        "  for query in queries:\n",
        "    output = base_models[base_model_names[model_index]](query)\n",
        "    model_responses.append(output)\n",
        "  return model_responses\n",
        "\n",
        "\n",
        "'''\n",
        "  ## The function \"createResponseFileForBaseModel\" collects the responses \n",
        "     returned by the \"getResponsesFromBaseModel\" and saves them in \n",
        "     the permanent file system.\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## base_models: is the list of all the base models provided by the MLMAC\n",
        "  ## base_model_names: is the names of all the base models provided by the MLMAC\n",
        "  ## queries: contains list of all assembled queries\n",
        "  ## model_index: is the index of the model whose responses are needed to get record\n",
        "\n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function will saves the list of responses generated by the specified\n",
        "     base model into a permanant file system\n",
        "'''\n",
        "def createResponseFileForBaseModel(model_index, base_model_names, base_models, queries):\n",
        "  model_responses = getResponsesFromBaseModel(model_index, base_model_names, base_models, queries)\n",
        "  with open(base_path + \"base_models/\" + base_model_names[model_index] + \"_api.json\", \"w\") as outfile:\n",
        "    outfile.write(json.dumps(model_responses, indent = 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we just use the model definition and the functions defined above to collect and save the responses generated by all the base models against each of the assembled queries."
      ],
      "metadata": {
        "id": "ZLQcWMWJ2q6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN WITH CAUTION\n",
        "\n",
        "HF_API_TOKEN = \"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\"\n"
      ],
      "metadata": {
        "id": "M6vnjII07VR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNMKBvj7LsXm"
      },
      "outputs": [],
      "source": [
        "## Use following link to access your hugging face api token \n",
        "## after creating your account\n",
        "##    - https://huggingface.co/settings/tokens\n",
        "  \n",
        "HF_API_TOKEN = \"enter_your_HF_API_TOKEN\"\n",
        "\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_models = {model_name: Model(\"hf\", HF_API_TOKEN, model_name, use_cache=False) for model_name in base_model_names}\n",
        "\n",
        "with open(base_path+\"queries.json\", 'r') as infile:\n",
        "  queries = json.load(infile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snbi2WRC2si3"
      },
      "outputs": [],
      "source": [
        "model_index = 7\n",
        "createResponseFileForBaseModel(model_index, base_model_names, base_models, queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COLLECTING RESPONSES FROM FINE-TUNED MODELS\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2p8KSITp5EgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uJ3TciOu4K_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## The function \"getResponsesFromFineTuneModels\" uses the provided list of \n",
        "     queries to interrogate the given fine-tuned model specified by the \n",
        "     \"finetuned_model\" parameter\n",
        "\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## finetuned_model:  fine-tuned model that needs to interrogate\n",
        "  ## queries: contains list of all assembled queries\n",
        "\n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function will return the list of responses generated by the specified\n",
        "     fine-tuned model for the given list of queries\n",
        "'''\n",
        "def getResponsesFromFineTuneModels(finetuned_model, queries):\n",
        "  model_responses = []\n",
        "  for query in queries:\n",
        "    output = finetuned_model(query)\n",
        "    model_responses.append(output)\n",
        "  return model_responses\n",
        "\n",
        "\n",
        "'''\n",
        "  ## The function \"createResponseFileForFineTunedModel\" collects the responses \n",
        "     returned by the \"getResponsesFromFineTuneModels\" and saves them in \n",
        "     the permanent file system.\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## ft_models: is the list of all the fine-tuned models provided by the MLMAC\n",
        "  ## queries: contains list of all assembled queries\n",
        "  ## model_index: is the index of the model whose responses are needed to get record\n",
        "\n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function will saves the list of responses generated by the specified\n",
        "     fine-tuned model into a permanant file system\n",
        "'''\n",
        "\n",
        "def createResponseFileForFineTunedModel(model_index, ft_models, queries):\n",
        "  model_responses = getResponsesFromFineTuneModels(ft_models[model_index], queries)\n",
        "  with open(base_path + \"finetuned_models/\" + str(model_index) + \"_api.json\", \"w\") as outfile:\n",
        "    outfile.write(json.dumps(model_responses, indent = 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we just use the model definition and the functions defined above to collect and save the responses generated by all the fine-tune models against each of the assembled queries."
      ],
      "metadata": {
        "id": "lQS-A2DXx0Vc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qJfEKC6x6fO"
      },
      "source": [
        "**RUN WITH CAUTION**\n",
        "\n",
        "MLMAC_API_TOKEN =  801c54ed1d254f83abf76d21654fc45d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uto5GOXQvSun"
      },
      "outputs": [],
      "source": [
        "## We have used the following link to create MLMAC api token \n",
        "## after creating our account\n",
        "##    - https://mlmac.io/status\n",
        "\n",
        "MLMAC_API_TOKEN = \"enter_your_MLMAC_API_TOKEN\"\n",
        "\n",
        "ft_models = [Model(\"mlmac\", MLMAC_API_TOKEN, idx) for idx in range(12)]\n",
        "\n",
        "with open(base_path+\"queries.json\", 'r') as infile:\n",
        "  queries = json.load(infile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tav6LdeawboJ"
      },
      "outputs": [],
      "source": [
        "model_index = 9\n",
        "\n",
        "if (not (MLMAC_API_TOKEN == \"\") and call_finetune_API and (model_index>-1)):\n",
        "  createResponseFileForFineTunedModel(model_index, ft_models, queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxcjrmkaFUzl"
      },
      "source": [
        "# APPROACH-01 BLEU & TER SCORES\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2BJ6k1rMVqk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## The function \"getBleuScore\" compares the given candidate sentences with\n",
        "     the provided reference sentence to generate a BLEU score\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## reference: It contains the actual expected output. In our case \n",
        "                it's the response generated by a fine-tuned model.\n",
        "  ## candidate: It contains the inferred output. In our case it's the response\n",
        "                generated by a base model.\n",
        "  \n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function returns a BLEU score between the provided \n",
        "     reference and candidate sentence.\n",
        "'''\n",
        "def getBleuScore(reference, candidate):\n",
        "  chencherry = SmoothingFunction()\n",
        "  return sentence_bleu([reference.split()], candidate.split(), smoothing_function=chencherry.method7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aksZHrVvW0OF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## The function \"getTerScore\" compares the given candidate sentences with\n",
        "     the provided reference sentence to generate a TER score\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## reference: It contains the actual expected output. In our case \n",
        "                it's the response generated by a fine-tuned model.\n",
        "  ## candidate: It contains the inferred output. In our case it's the response\n",
        "                generated by a base model.\n",
        "  \n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function returns a TER score between the provided \n",
        "     reference and candidate sentence.\n",
        "'''\n",
        "\n",
        "def getTerScore(reference, candidate):\n",
        "  return pyter.ter(candidate.split(), reference.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y_v3aodADxT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## The function \"remove_punctuations\" removes punctuations from the input sentence\n",
        "\n",
        "  ## FUNCTION INPUT\n",
        "  ## sentence: A string from which punctuations needs to get removed.\n",
        "  \n",
        "  ## FUNCTION OUTPUT\n",
        "  ## The function outputs a string from which punctuations after removing \n",
        "     all punctuation marks from it\n",
        "'''\n",
        "\n",
        "def remove_punctuations(sentence):\n",
        "    sentence_w_multi_line = \" \".join(sentence.splitlines())\n",
        "    sentence_w_punct = \"\".join([i.lower() if i not in string.punctuation else \" \" for i in sentence_w_multi_line])\n",
        "    sentence = sentence_w_punct.strip()\n",
        "    #sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will use the above-developed functions to pair the fine-tuned models with a base model that is most suitable for its generated responses."
      ],
      "metadata": {
        "id": "6zilTvG35yDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PdzFUC7TbN7w"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## NAMES OF ALL BASE MODELS\n",
        "'''\n",
        "\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "'''\n",
        "  ## EXECUTION OF APPROACH-01\n",
        "'''\n",
        "scoring = {}\n",
        "for index in range(0, 12):\n",
        "  model_index = index\n",
        "  \n",
        "  with open(base_path+ \"finetuned_models/\" + str(model_index) + \"_api.json\", 'r') as infile:\n",
        "    finetuned_model_responses = json.load(infile)\n",
        "\n",
        "  bleu_matching = {}\n",
        "  ter_matching = {}\n",
        "\n",
        "  bleu_matching_with_punct = {}\n",
        "  ter_matching_with_punct = {}\n",
        "\n",
        "  for base_model in base_model_names:\n",
        "    base_model_response_path = base_path+ \"base_models/\"\n",
        "    \n",
        "    if base_model in base_model_api_responses:\n",
        "      base_model_response_path +=  base_model + \"_api.json\"\n",
        "    else:\n",
        "      base_model_response_path +=  \"model_\" + base_model + \".json\"\n",
        "\n",
        "    with open(base_model_response_path, 'r') as infile:\n",
        "      base_model_responses = json.load(infile)\n",
        "    \n",
        "    bleu_scores = []\n",
        "    ter_scores = []\n",
        "\n",
        "    bleu_scores_with_punct = []\n",
        "    ter_scores_with_punct = []\n",
        "\n",
        "    for (base_model_resp, finetuned_model_resp) in zip(base_model_responses, finetuned_model_responses):\n",
        "      \n",
        "      ref = base_model_resp[0]['generated_text']\n",
        "      cand = finetuned_model_resp['generated_text']\n",
        "\n",
        "      bleu_scores_with_punct.append(getBleuScore(ref, cand))\n",
        "      ter_scores_with_punct.append(getTerScore(ref, cand))\n",
        "      \n",
        "      ref = remove_punctuations(ref)\n",
        "      cand = remove_punctuations(cand)\n",
        "      \n",
        "      bleu_scores.append(getBleuScore(ref, cand))\n",
        "      ter_scores.append(getTerScore(ref, cand))\n",
        "\n",
        "      print(ref, \",\", cand)\n",
        "      \n",
        "\n",
        "\n",
        "    bleu_matching[base_model] = sum(bleu_scores) / len(bleu_scores)\n",
        "    ter_matching[base_model] = sum(ter_scores) / len(ter_scores)\n",
        "\n",
        "    bleu_matching_with_punct[base_model] = sum(bleu_scores_with_punct) / len(bleu_scores_with_punct) \n",
        "    ter_matching_with_punct[base_model] = sum(ter_scores_with_punct) / len(ter_scores_with_punct)\n",
        "    \n",
        "    scoring[model_index] = {}\n",
        "    scoring[model_index][\"bleu_matching\"] = dict(sorted(bleu_matching.items(), key=lambda item: item[1]))\n",
        "    scoring[model_index][\"ter_matching\"] = dict(sorted(ter_matching.items(), key=lambda item: item[1]))\n",
        "    scoring[model_index][\"bleu_matching_with_punct\"] = dict(sorted(bleu_matching_with_punct.items(), key=lambda item: item[1]))\n",
        "    scoring[model_index][\"ter_matching_with_punct\"] = dict(sorted(ter_matching_with_punct.items(), key=lambda item: item[1]))\n",
        "\n",
        "    scoring[model_index][\"bleu_matching_resp\"] = max(bleu_matching, key=bleu_matching.get)\n",
        "    scoring[model_index][\"ter_matching_resp\"] = min(ter_matching, key=ter_matching.get)\n",
        "    scoring[model_index][\"bleu_matching_with_punct_resp\"] = max(bleu_matching_with_punct, key=bleu_matching_with_punct.get)\n",
        "    scoring[model_index][\"ter_matching_with_punct_resp\"] = min(ter_matching_with_punct, key=ter_matching_with_punct.get)\n",
        "\n",
        "'''\n",
        "  ## SAVING ALL THE CALCULATIONS IN FILE-SYSTEM\n",
        "'''\n",
        "with open(base_path + \"bleu_ter_scoring.json\", \"w\") as outfile:\n",
        "  outfile.write(json.dumps(scoring, indent = 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GENERATING FINAL PAIRING\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9NS_Tod56hWa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw7stiijcE6z"
      },
      "outputs": [],
      "source": [
        "dict(sorted(bleu_matching.items(), key=lambda item: item[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1nmdpJkhj23"
      },
      "outputs": [],
      "source": [
        "dict(sorted(ter_matching.items(), key=lambda item: item[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixmroaOHCbLG"
      },
      "outputs": [],
      "source": [
        "dict(sorted(bleu_matching_with_punct.items(), key=lambda item: item[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js7t4y_LCcnZ"
      },
      "outputs": [],
      "source": [
        "dict(sorted(ter_matching_with_punct.items(), key=lambda item: item[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9u2s7zhhywl"
      },
      "outputs": [],
      "source": [
        "max(bleu_matching, key=bleu_matching.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0J7Lwrch3aP"
      },
      "outputs": [],
      "source": [
        "min(ter_matching, key=ter_matching.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYB6VPghFbtp"
      },
      "source": [
        "# APPROACH-02 VSM MODEL\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VSM MODEL VANILLA DOCUMENT-DOCUMENT BASED APPROACH\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CjgHOtJgAqlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, we have utilized the basic implementation of the VSM model to pair the most suitable base model with a fine-tuned model. In this implementation, we have created documents for each fine-tuned and base model by concatenating their generated responses in identically comparable order. And then pair the fine-tuned model with the base model whose generated documents are most similar in the vector space."
      ],
      "metadata": {
        "id": "1kJnfIpvBWJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5csbXZ9kT9y6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## NAMES OF ALL BASE MODELS\n",
        "'''\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "docs = pd.Series()\n",
        "\n",
        "'''\n",
        "  ## BUILDING DOCUMENTS FOR EACH FINE-TUNED MODEL\n",
        "'''\n",
        "\n",
        "for file_index in range(0, 12):\n",
        "  model_index = file_index\n",
        "  doc_name = \"ft_\"+str(model_index)\n",
        "  with open(base_path+ \"finetuned_models/\" + str(model_index) + \"_api.json\", 'r') as infile:\n",
        "    finetuned_model_responses = json.load(infile)\n",
        "  \n",
        "  all_resp = \"\"\n",
        "  for index in range(0, len(finetuned_model_responses)):\n",
        "    all_resp += finetuned_model_responses[index]['generated_text'] + \"\\n\"\n",
        "  docs[doc_name] = all_resp\n",
        "\n",
        "\n",
        "'''\n",
        "  ## GENERATING DOCUMENTS FOR EACH BASE MODEL\n",
        "'''\n",
        "\n",
        "for base_model in base_model_names:\n",
        "  base_model_response_path = base_path+ \"base_models/\"    \n",
        "  if base_model in base_model_api_responses:\n",
        "    base_model_response_path +=  base_model + \"_api.json\"\n",
        "  else:\n",
        "    base_model_response_path +=  \"model_\" + base_model + \".json\"\n",
        "\n",
        "  with open(base_model_response_path, 'r') as infile:\n",
        "    base_model_responses = json.load(infile)\n",
        "  \n",
        "  all_resp = \"\"\n",
        "  for index in range(0, len(base_model_responses)):\n",
        "    response = base_model_responses[index][0]['generated_text']\n",
        "    all_resp += response + \"\\n\"\n",
        "  docs[base_model] = all_resp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GENERATING FINAL PAIRING \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "l0BCc8ZLAVxi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMOPy3tYutdQ"
      },
      "outputs": [],
      "source": [
        "vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
        "vec.fit(docs) # This determines the vocabulary.\n",
        "tf_idf_sparse = vec.transform(docs)\n",
        "cos_sim = cosine_similarity(tf_idf_sparse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXXgehCWnwMO"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[0][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIQMUkYfn7Ip"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[1][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w05bfgdyoywF"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[2][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PizeYN0urQd3"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[3][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXdhs7QnrVEf"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[4][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LopUxHiqrWIr"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[5][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPo9TK7xrZ3V"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[6][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOKAD4sfrbvX"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[7][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHs8JXXDrd6n"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[8][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nOJutiwrgJ2"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[9][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jufgx7tJrh3o"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[10][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "javjn0tkrjt8"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[11][12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYu8uR3jE5r1"
      },
      "outputs": [],
      "source": [
        "np.argmax(cos_sim[-1][12:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMTnRXY0AS9X"
      },
      "source": [
        "## VSM MODEL CROSS VALIDATION\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in this sub-section, we have utilized the ten-fold cross-validation approach to analyze the performance of the vector space model by artificially simulating the pairing of the base model with the fine-tuned model. We have created a synthetic data set from the responses of the base models and recorded the overall accuracy of the approach in each fold. "
      ],
      "metadata": {
        "id": "frc0ytDdG6mK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywIgLeaynCOU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## NAMES OF THE BASE MODEL\n",
        "'''\n",
        "\n",
        "\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "\n",
        "'''\n",
        "  ## PREPARING SYNTHETIC DATA SET FROM THE RESPONSES OF BASE MODEL\n",
        "'''\n",
        "\n",
        "labels = []\n",
        "responses = []\n",
        "model_index = 0\n",
        "\n",
        "for base_model in base_model_names:\n",
        "  base_model_response_path = base_path+ \"base_models/\"    \n",
        "  if base_model in base_model_api_responses:\n",
        "    base_model_response_path +=  base_model + \"_api.json\"\n",
        "  else:\n",
        "    base_model_response_path +=  \"model_\" + base_model + \".json\"\n",
        "\n",
        "  with open(base_model_response_path, 'r') as infile:\n",
        "    base_model_responses = json.load(infile)\n",
        "  \n",
        "  for index in range(0, len(base_model_responses)):\n",
        "    resp = base_model_responses[index][0]['generated_text']\n",
        "    responses.append(resp)\n",
        "    labels.append(model_index)\n",
        "  \n",
        "  model_index += 1\n",
        "\n",
        "df = pd.DataFrame([responses, labels], index=['responses', 'labels']).transpose()\n",
        "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
        "\n",
        "\n",
        "'''\n",
        "  ## EXECUTION OF K-FOLD VALIDATION\n",
        "'''\n",
        "folds = 0\n",
        "\n",
        "for train_index, test_index in kf.split(df):\n",
        "  \n",
        "  train = df.iloc[train_index]\n",
        "  test = df.iloc[test_index]\n",
        "  print(\" ============== FOLD\",folds,\" ==============\")\n",
        "  \n",
        "  correct_pred_fold_avg = 0\n",
        "  correct_pred_fold_inst = 0\n",
        "  correct_pred_fold_freq = 0\n",
        "  total_insts = 0\n",
        "  correct_pred_fold_inst_total = 0\n",
        "\n",
        "\n",
        "  docs = pd.Series()\n",
        "  count_freq_pred = [0]*12\n",
        "\n",
        "  for b_labels in range(0, 12):\n",
        "    all_resp = \"\"\n",
        "    for index, record in train[train.labels==b_labels].iterrows():\n",
        "      resp = record.responses.strip()\n",
        "      resp = resp.replace(\"\\n\", \" \")\n",
        "      all_resp = resp + \"\\n\"\n",
        "    \n",
        "    docs[str(b_labels)] = all_resp\n",
        "\n",
        "  for ft_label in range(0, 12):\n",
        "    \n",
        "    aggregator = np.zeros(12)\n",
        "    for index, record in test[test.labels==ft_label].iterrows():\n",
        "      resp = record.responses.strip()\n",
        "      resp = resp.replace(\"\\n\", \" \")\n",
        "\n",
        "      newDocs = docs.copy()\n",
        "      newDocs[str(ft_label)+\"-\"+str(index)] = resp\n",
        "      vec = TfidfVectorizer(norm=None) \n",
        "      vec.fit(newDocs) \n",
        "      \n",
        "      tf_idf_sparse = vec.transform(newDocs)\n",
        "      cos_sim = cosine_similarity(tf_idf_sparse)\n",
        "      aggregator += cos_sim[12][:12]\n",
        "      \n",
        "      pred_label_record = np.argmax(cos_sim[12][:12])\n",
        "      count_freq_pred[pred_label_record]+=1\n",
        "\n",
        "      if(pred_label_record == record.labels):\n",
        "        correct_pred_fold_inst +=1\n",
        "\n",
        "    \n",
        "    aggregator = aggregator/len(test[test.labels==ft_label])\n",
        "    pred_label_via_avg = np.argmax(aggregator)\n",
        "\n",
        "    pred_label_via_max_freq = np.argmax(count_freq_pred)\n",
        "\n",
        "    print(str(ft_label) + \"<=> avg\", str(pred_label_via_avg))\n",
        "    print(str(ft_label) + \"<=> freq\", str(pred_label_via_max_freq))\n",
        "\n",
        "    print(\"Total Correctly predicted Instances =\",correct_pred_fold_inst, \"out of\", len(test[test.labels==ft_label]), \"when label is \", ft_label)\n",
        "    correct_pred_fold_inst_total += correct_pred_fold_inst\n",
        "    total_insts+=len(test[test.labels==ft_label])\n",
        "\n",
        "    if(ft_label==pred_label_via_avg):\n",
        "      correct_pred_fold_avg +=1\n",
        "    \n",
        "    if (ft_label == pred_label_via_max_freq):\n",
        "      correct_pred_fold_freq +=1\n",
        "\n",
        "    \n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds, \"for FT models via avg =\", correct_pred_fold_avg, \"out of 12\")\n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds, \"for FT models via max freq =\", correct_pred_fold_freq, \"out of 12\")\n",
        "  print(\"Total Correctly predicted instances in FOLD =\", folds, \"for individual responses of FT model\", correct_pred_fold_inst_total, \"out of \", total_insts)\n",
        "  folds +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VSM MODEL VANILLA DOCUMENT-QUERY BASED APPROACH\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wtBba5KlI72l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this implementation, we have utilized the basic implementation of the VSM model to pair the most suitable base model with a fine-tuned model. In this implementation, we have created documents for each base model by concatenating their generated responses in identically comparable order. We have then created a list of responses for each of the given fine-tuned models. Later, we used each response generated by the fine-tuned models as a query and paired it with the most relevant document in the constructed VSM. In the end, we have paired the fine-tuned models with the base model whose generated document got most frequently got paired with its generated responses."
      ],
      "metadata": {
        "id": "sQwAK5oZXXfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op7VTPbXrlLo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## NAMES OF THE BASE MODEL\n",
        "'''\n",
        "\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "'''\n",
        "  ## GENERATING DOCUMENTS FOR EACH BASE MODEL\n",
        "'''\n",
        "\n",
        "docs = pd.Series()\n",
        "\n",
        "for base_model in base_model_names:\n",
        "  base_model_response_path = base_path+ \"base_models/\"    \n",
        "  if base_model in base_model_api_responses:\n",
        "    base_model_response_path +=  base_model + \"_api.json\"\n",
        "  else:\n",
        "    base_model_response_path +=  \"model_\" + base_model + \".json\"\n",
        "\n",
        "  with open(base_model_response_path, 'r') as infile:\n",
        "    base_model_responses = json.load(infile)\n",
        "  \n",
        "  all_resp = \"\"\n",
        "  for index in range(0, len(base_model_responses)):\n",
        "    response = base_model_responses[index][0]['generated_text']\n",
        "    all_resp += response + \"\\n\"\n",
        "  docs[base_model] = all_resp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GENERATING FINAL PAIRING\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ULrGT1SgX-_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  ## EXECUTION OF THE APPROACH-02\n",
        "'''\n",
        "for file_index in range(0, 12):\n",
        "  model_index = file_index\n",
        "  doc_name = \"ft_\"+str(model_index)\n",
        "  if (file_index==9):\n",
        "    model_index = 8\n",
        "  with open(base_path+ \"finetuned_models/\" + str(model_index) + \"_api.json\", 'r') as infile:\n",
        "    finetuned_model_responses = json.load(infile)\n",
        "  \n",
        "  aggregator = np.zeros(12)\n",
        "  for index in range(0, len(finetuned_model_responses)):\n",
        "    resp = finetuned_model_responses[index]['generated_text']\n",
        "    newDocs = docs.copy()\n",
        "    newDocs[doc_name+str(index)] = resp\n",
        "\n",
        "    vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
        "    vec.fit(newDocs) # This determines the vocabulary.\n",
        "    tf_idf_sparse = vec.transform(newDocs)\n",
        "    cos_sim = cosine_similarity(tf_idf_sparse)\n",
        "    aggregator += cos_sim[12][:12]\n",
        "\n",
        "  aggregator = aggregator/len(finetuned_model_responses)\n",
        "  print(str(model_index) + \"<=>\", np.argmax(aggregator))"
      ],
      "metadata": {
        "id": "Lf5PB6JHX9fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-XPd1q6lpZi"
      },
      "source": [
        "# APRROACH-03 MULTI-CLASS TEXT CLASSIFICATION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx5pjrHLIco7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## NAMES OF THE BASE MODEL\n",
        "'''\n",
        "\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "\n",
        "'''\n",
        "  ## POPULATING LIST OF RESPONSES FOR EACH BASE MODEL\n",
        "'''\n",
        "docs = pd.Series()\n",
        "\n",
        "labels = []\n",
        "responses = []\n",
        "model_index = 0\n",
        "\n",
        "for base_model in base_model_names:\n",
        "  base_model_response_path = base_path+ \"base_models/\"    \n",
        "  if base_model in base_model_api_responses:\n",
        "    base_model_response_path +=  base_model + \"_api.json\"\n",
        "  else:\n",
        "    base_model_response_path +=  \"model_\" + base_model + \".json\"\n",
        "\n",
        "  with open(base_model_response_path, 'r') as infile:\n",
        "    base_model_responses = json.load(infile)\n",
        "  \n",
        "  for index in range(0, len(base_model_responses)):\n",
        "    resp = base_model_responses[index][0]['generated_text']\n",
        "    responses.append(resp)\n",
        "    labels.append(model_index)\n",
        "  \n",
        "  model_index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1k_WxNuGscf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## CONSTRUCTING DATA FRAME FOR DEVELOPING TRAINING DATA SET\n",
        "'''\n",
        "\n",
        "df = pd.DataFrame([responses, labels], index=['responses', 'labels']).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0kSySnjL_Uo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## CONSTRUCTING LIST OF MODEL NAMES\n",
        "'''\n",
        "\n",
        "model_names = [\"distilbert-base-uncased\", 'google/electra-small-discriminator', 'albert-base-v2', \"roberta-base\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATKYGlWizVJW"
      },
      "source": [
        "## 10-FOLD CROSS VALIDATION USING DISTIL-BERT\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU3pxruVs_Oz"
      },
      "outputs": [],
      "source": [
        "model_name = model_names[0]\n",
        "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
        "accuracy = []\n",
        "folds = kf.split(df)\n",
        "\n",
        "\n",
        "correct_pred_fold_avg = 0\n",
        "correct_pred_fold_freq = 0\n",
        "folds_no = 0\n",
        "\n",
        "for train_index, test_index  in folds:\n",
        "  train = df.iloc[train_index]\n",
        "  test = df.iloc[test_index]\n",
        "\n",
        "  dataset = Dataset.from_pandas(train)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenized_dataset = dataset.map((lambda x: tokenizer(x[\"responses\"], truncation=True, max_length=647)), batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "  tf_train_set = tokenized_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "  )\n",
        "\n",
        "\n",
        "  batch_size = 16\n",
        "  num_epochs = 3\n",
        "  batches_per_epoch = len(tokenized_dataset) // batch_size\n",
        "  total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "  optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "  model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=12)\n",
        "  model.compile(optimizer=optimizer)\n",
        "  model.fit(x=tf_train_set, epochs=num_epochs)\n",
        "\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  for ft_label in range(0, 12):\n",
        "    \n",
        "    aggregator = np.zeros(12)\n",
        "    frequencer = np.zeros(12)\n",
        "    correct_pred_fold_inst = 0\n",
        "\n",
        "    for index, record in test[test.labels==ft_label].iterrows():\n",
        "      resp  = record['responses']\n",
        "      label = record['labels']\n",
        "\n",
        "      tokenized = tokenizer(resp, return_tensors=\"np\", padding=\"longest\", truncation=True)\n",
        "      output = model(tokenized).logits\n",
        "      model_pred = np.argmax(output, axis=1)\n",
        "      aggregator+=output.numpy()[0]\n",
        "      frequencer[model_pred]+=1\n",
        "\n",
        "      classifications.append(model_pred)\n",
        "      labels.append(ft_label)\n",
        "\n",
        "      if(model_pred == ft_label):\n",
        "        correct_pred_fold_inst+=1\n",
        "\n",
        "    pred_label_via_avg = np.argmax(aggregator/len(test[test.labels==ft_label]))\n",
        "    pred_label_via_max_freq = np.argmax(frequencer)\n",
        "\n",
        "    print(str(ft_label) + \"<=> avg\", str(pred_label_via_avg))\n",
        "    print(str(ft_label) + \"<=> freq\", str(pred_label_via_max_freq))\n",
        "    print(\"Total Correctly predicted Instances =\",correct_pred_fold_inst, \"out of\", len(test[test.labels==ft_label]), \"when label is \", ft_label)\n",
        "\n",
        "    if(ft_label==pred_label_via_avg):\n",
        "      correct_pred_fold_avg +=1\n",
        "    \n",
        "    if (ft_label == pred_label_via_max_freq):\n",
        "      correct_pred_fold_freq +=1\n",
        "\n",
        "    \n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via avg =\", correct_pred_fold_avg, \"out of 12\")\n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via max freq =\", correct_pred_fold_freq, \"out of 12\")\n",
        "  accuracy.append(((classifications == np.array(labels)).sum() / len(test)))\n",
        "  print(accuracy)\n",
        "  folds_no+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10-FOLD CROSS VALIDATION USING ELECTRA-SMALL\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yW12G7H4j1ie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmC3IPqJg07b"
      },
      "outputs": [],
      "source": [
        "model_name = model_names[1]\n",
        "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
        "accuracy = []\n",
        "folds = kf.split(df)\n",
        "\n",
        "\n",
        "correct_pred_fold_avg = 0\n",
        "correct_pred_fold_freq = 0\n",
        "folds_no = 0\n",
        "\n",
        "for train_index, test_index  in folds:\n",
        "  train = df.iloc[train_index]\n",
        "  test = df.iloc[test_index]\n",
        "\n",
        "  dataset = Dataset.from_pandas(train)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenized_dataset = dataset.map((lambda x: tokenizer(x[\"responses\"], truncation=True, max_length=647)), batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "  tf_train_set = tokenized_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "  )\n",
        "\n",
        "\n",
        "  batch_size = 16\n",
        "  num_epochs = 3\n",
        "  batches_per_epoch = len(tokenized_dataset) // batch_size\n",
        "  total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "  optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "  model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=12)\n",
        "  model.compile(optimizer=optimizer)\n",
        "  model.fit(x=tf_train_set, epochs=num_epochs)\n",
        "\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  for ft_label in range(0, 12):\n",
        "    \n",
        "    aggregator = np.zeros(12)\n",
        "    frequencer = np.zeros(12)\n",
        "    correct_pred_fold_inst = 0\n",
        "\n",
        "    for index, record in test[test.labels==ft_label].iterrows():\n",
        "      resp  = record['responses']\n",
        "      label = record['labels']\n",
        "\n",
        "      tokenized = tokenizer(resp, return_tensors=\"np\", padding=\"longest\", truncation=True)\n",
        "      output = model(tokenized).logits\n",
        "      model_pred = np.argmax(output, axis=1)\n",
        "      aggregator+=output.numpy()[0]\n",
        "      frequencer[model_pred]+=1\n",
        "\n",
        "      classifications.append(model_pred)\n",
        "      labels.append(ft_label)\n",
        "\n",
        "      if(model_pred == ft_label):\n",
        "        correct_pred_fold_inst+=1\n",
        "\n",
        "    pred_label_via_avg = np.argmax(aggregator/len(test[test.labels==ft_label]))\n",
        "    pred_label_via_max_freq = np.argmax(frequencer)\n",
        "\n",
        "    print(str(ft_label) + \"<=> avg\", str(pred_label_via_avg))\n",
        "    print(str(ft_label) + \"<=> freq\", str(pred_label_via_max_freq))\n",
        "    print(\"Total Correctly predicted Instances =\",correct_pred_fold_inst, \"out of\", len(test[test.labels==ft_label]), \"when label is \", ft_label)\n",
        "\n",
        "    if(ft_label==pred_label_via_avg):\n",
        "      correct_pred_fold_avg +=1\n",
        "    \n",
        "    if (ft_label == pred_label_via_max_freq):\n",
        "      correct_pred_fold_freq +=1\n",
        "\n",
        "    \n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via avg =\", correct_pred_fold_avg, \"out of 12\")\n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via max freq =\", correct_pred_fold_freq, \"out of 12\")\n",
        "  accuracy.append(((classifications == np.array(labels)).sum() / len(test)))\n",
        "  print(accuracy)\n",
        "  folds_no+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10-FOLD CROSS VALIDATION USING ALBERT-BASE\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Jejf4EFfj9rh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K_vO-XcGoKqR"
      },
      "outputs": [],
      "source": [
        "model_name = model_names[2]\n",
        "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
        "accuracy = []\n",
        "folds = kf.split(df)\n",
        "\n",
        "\n",
        "correct_pred_fold_avg = 0\n",
        "correct_pred_fold_freq = 0\n",
        "folds_no = 0\n",
        "\n",
        "for train_index, test_index  in folds:\n",
        "  train = df.iloc[train_index]\n",
        "  test = df.iloc[test_index]\n",
        "\n",
        "  dataset = Dataset.from_pandas(train)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenized_dataset = dataset.map((lambda x: tokenizer(x[\"responses\"], truncation=True, max_length=647)), batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "  tf_train_set = tokenized_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "  )\n",
        "\n",
        "\n",
        "  batch_size = 16\n",
        "  num_epochs = 3\n",
        "  batches_per_epoch = len(tokenized_dataset) // batch_size\n",
        "  total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "  optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "  model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=12)\n",
        "  model.compile(optimizer=optimizer)\n",
        "  model.fit(x=tf_train_set, epochs=num_epochs)\n",
        "\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  for ft_label in range(0, 12):\n",
        "    \n",
        "    aggregator = np.zeros(12)\n",
        "    frequencer = np.zeros(12)\n",
        "    correct_pred_fold_inst = 0\n",
        "\n",
        "    for index, record in test[test.labels==ft_label].iterrows():\n",
        "      resp  = record['responses']\n",
        "      label = record['labels']\n",
        "\n",
        "      tokenized = tokenizer(resp, return_tensors=\"np\", padding=\"longest\", truncation=True)\n",
        "      output = model(tokenized).logits\n",
        "      model_pred = np.argmax(output, axis=1)\n",
        "      aggregator+=output.numpy()[0]\n",
        "      frequencer[model_pred]+=1\n",
        "\n",
        "      classifications.append(model_pred)\n",
        "      labels.append(ft_label)\n",
        "\n",
        "      if(model_pred == ft_label):\n",
        "        correct_pred_fold_inst+=1\n",
        "\n",
        "    pred_label_via_avg = np.argmax(aggregator/len(test[test.labels==ft_label]))\n",
        "    pred_label_via_max_freq = np.argmax(frequencer)\n",
        "\n",
        "    print(str(ft_label) + \"<=> avg\", str(pred_label_via_avg))\n",
        "    print(str(ft_label) + \"<=> freq\", str(pred_label_via_max_freq))\n",
        "    print(\"Total Correctly predicted Instances =\",correct_pred_fold_inst, \"out of\", len(test[test.labels==ft_label]), \"when label is \", ft_label)\n",
        "\n",
        "    if(ft_label==pred_label_via_avg):\n",
        "      correct_pred_fold_avg +=1\n",
        "    \n",
        "    if (ft_label == pred_label_via_max_freq):\n",
        "      correct_pred_fold_freq +=1\n",
        "\n",
        "    \n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via avg =\", correct_pred_fold_avg, \"out of 12\")\n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via max freq =\", correct_pred_fold_freq, \"out of 12\")\n",
        "  accuracy.append(((classifications == np.array(labels)).sum() / len(test)))\n",
        "  print(accuracy)\n",
        "  folds_no+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10-FOLD CROSS VALIDATION USING ROBERTA-BASE\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XtsJVzC3kXg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_names[3]\n",
        "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
        "accuracy = []\n",
        "folds = kf.split(df)\n",
        "\n",
        "\n",
        "correct_pred_fold_avg = 0\n",
        "correct_pred_fold_freq = 0\n",
        "folds_no = 0\n",
        "\n",
        "for train_index, test_index  in folds:\n",
        "  train = df.iloc[train_index]\n",
        "  test = df.iloc[test_index]\n",
        "\n",
        "  dataset = Dataset.from_pandas(train)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenized_dataset = dataset.map((lambda x: tokenizer(x[\"responses\"], truncation=True, max_length=647)), batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "  tf_train_set = tokenized_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "  )\n",
        "\n",
        "\n",
        "  batch_size = 16\n",
        "  num_epochs = 3\n",
        "  batches_per_epoch = len(tokenized_dataset) // batch_size\n",
        "  total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "  optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "  model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=12)\n",
        "  model.compile(optimizer=optimizer)\n",
        "  model.fit(x=tf_train_set, epochs=num_epochs)\n",
        "\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  for ft_label in range(0, 12):\n",
        "    \n",
        "    aggregator = np.zeros(12)\n",
        "    frequencer = np.zeros(12)\n",
        "    correct_pred_fold_inst = 0\n",
        "\n",
        "    for index, record in test[test.labels==ft_label].iterrows():\n",
        "      resp  = record['responses']\n",
        "      label = record['labels']\n",
        "\n",
        "      tokenized = tokenizer(resp, return_tensors=\"np\", padding=\"longest\", truncation=True)\n",
        "      output = model(tokenized).logits\n",
        "      model_pred = np.argmax(output, axis=1)\n",
        "      aggregator+=output.numpy()[0]\n",
        "      frequencer[model_pred]+=1\n",
        "\n",
        "      classifications.append(model_pred)\n",
        "      labels.append(ft_label)\n",
        "\n",
        "      if(model_pred == ft_label):\n",
        "        correct_pred_fold_inst+=1\n",
        "\n",
        "    pred_label_via_avg = np.argmax(aggregator/len(test[test.labels==ft_label]))\n",
        "    pred_label_via_max_freq = np.argmax(frequencer)\n",
        "\n",
        "    print(str(ft_label) + \"<=> avg\", str(pred_label_via_avg))\n",
        "    print(str(ft_label) + \"<=> freq\", str(pred_label_via_max_freq))\n",
        "    print(\"Total Correctly predicted Instances =\",correct_pred_fold_inst, \"out of\", len(test[test.labels==ft_label]), \"when label is \", ft_label)\n",
        "\n",
        "    if(ft_label==pred_label_via_avg):\n",
        "      correct_pred_fold_avg +=1\n",
        "    \n",
        "    if (ft_label == pred_label_via_max_freq):\n",
        "      correct_pred_fold_freq +=1\n",
        "\n",
        "    \n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via avg =\", correct_pred_fold_avg, \"out of 12\")\n",
        "  print(\"Total Correctly predicted labels in FOLD\", folds_no, \"for FT models via max freq =\", correct_pred_fold_freq, \"out of 12\")\n",
        "  accuracy.append(((classifications == np.array(labels)).sum() / len(test)))\n",
        "  print(accuracy)\n",
        "  folds_no+=1"
      ],
      "metadata": {
        "id": "K18Yiby4oJCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GENERATING FINAL PAIRING\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PpY9_7prwbKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t82Kuvr6I4A0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## MODEL TRAINING\n",
        "'''\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "model_name =  'google/electra-small-discriminator' #'google/electra-small-discriminator' #'albert-base-v2' #\"roberta-base\" #distilbert-base-uncased\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"responses\"], truncation=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "tf_train_set = tokenized_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "batches_per_epoch = len(tokenized_dataset) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=12)\n",
        "model.compile(optimizer=optimizer)\n",
        "model.fit(x=tf_train_set, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASsX84GcI5qa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## SAVING THE TRAINED MODEL TO FILE SYSTEM\n",
        "'''\n",
        "\n",
        "model.save_pretrained (base_path+ \"FT/\"+model_name)\n",
        "tokenizer.save_pretrained(base_path+ \"FT/\"+model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVD5hLF5OQLe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## LOADIG BACK THE SAVED TRAINED MODEL FROM FILE SYSTEM\n",
        "'''\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(base_path+ \"FT/\"+model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_path+ \"FT/\"+model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIFSsDkdOpJc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ## LOADIG BACK THE SAVED TRAINED MODEL FROM FILE SYSTEM & GENERATING PAIRING\n",
        "'''\n",
        "\n",
        "for file_index in range(0, 12):\n",
        "  model_index = file_index\n",
        "  doc_name = \"ft_\"+str(model_index)\n",
        "  with open(base_path+ \"finetuned_models/\" + str(model_index) + \"_api.json\", 'r') as infile:\n",
        "    finetuned_model_responses = json.load(infile)\n",
        "  \n",
        "  responses = []\n",
        "  for index in range(0, len(finetuned_model_responses)):\n",
        "    resp = finetuned_model_responses[index]['generated_text']\n",
        "    responses.append(resp)\n",
        "\n",
        "  tokenized = tokenizer(responses, return_tensors=\"np\", padding=\"longest\")\n",
        "\n",
        "  outputs = model(tokenized).logits\n",
        "\n",
        "  classifications = np.argmax(outputs, axis=1)\n",
        "  print(\"freq\", np.argmax(np.bincount(classifications)), \"==== avg\", np.argmax(tf.reduce_sum(outputs, 0).numpy()/100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPROACH-04 MULTI-CLASS TEXT CLASSIFICATION VIA ONE-VS-ALL"
      ],
      "metadata": {
        "id": "2CdLCACuyXFm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr-m3JrFdZOT"
      },
      "outputs": [],
      "source": [
        "with open(base_path+\"queries.json\", 'r') as infile:\n",
        "  queries = json.load(infile)\n",
        "\n",
        "for file_index in range(0, 1):\n",
        "  model_index = file_index\n",
        "  doc_name = \"ft_\"+str(model_index)\n",
        "  if (file_index==9):\n",
        "    model_index = 8\n",
        "  with open(base_path+ \"finetuned_models/\" + str(model_index) + \"_api.json\", 'r') as infile:\n",
        "    finetuned_model_responses = json.load(infile)\n",
        "  \n",
        "  responses = []\n",
        "  iit=0\n",
        "  for index in range(0, len(finetuned_model_responses)):\n",
        "    resp = finetuned_model_responses[index]['generated_text']\n",
        "    if (not(resp in queries)):\n",
        "      print(resp)\n",
        "      iit +=1\n",
        "  print(iit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-4QkD4U5Etz"
      },
      "outputs": [],
      "source": [
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "\n",
        "base_model = base_model_names[0]\n",
        "if base_model in base_model_api_responses:\n",
        "  base_model_response_path = base_path+ \"base_models/\"+base_model + \"_api.json\"\n",
        "else:\n",
        "  base_model_response_path = base_path+ \"base_models/\"+  \"model_\" + base_model+\".json\"\n",
        "\n",
        "with open(base_model_response_path, 'r') as infile:\n",
        "    base_model_responses = json.load(infile)\n",
        "\n",
        "model_index = 11\n",
        "doc_name = \"ft_\"+str(model_index)\n",
        "with open(base_path+ \"finetuned_models/\" + str(model_index) + \"_api.json\", 'r') as infile:\n",
        "  finetuned_model_responses = json.load(infile)\n",
        "\n",
        "for base_resp, fine_tune_resp in zip(base_model_responses, finetuned_model_responses):\n",
        "  if (fine_tune_resp['generated_text'] == base_resp[0]['generated_text']):\n",
        "    print(fine_tune_resp['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GCe4HOoe2TA"
      },
      "outputs": [],
      "source": [
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_model_api_responses = [\"bloom-2b5\", \"gpt2-xl\", \"gpt-j-6B\", \"xlnet-base-cased\"]\n",
        "\n",
        "\n",
        "\n",
        "base_model = \"DialoGPT-large\"\n",
        "base_model_response_path = base_path+ \"base_models/\"    \n",
        "if base_model in base_model_api_responses:\n",
        "  base_model_response_path +=  base_model + \"_api.json\"\n",
        "else:\n",
        "  base_model_response_path +=  \"model_\" + base_model + \".json\"\n",
        "\n",
        "with open(base_model_response_path, 'r') as infile:\n",
        "  base_model_responses = json.load(infile)\n",
        "\n",
        "iit = 0\n",
        "for index in range(0, len(base_model_responses)):\n",
        "  response = base_model_responses[index][0]['generated_text']\n",
        "  if (not(response in queries)):\n",
        "    print(response)\n",
        "    iit +=1\n",
        "print(iit)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMG+F/lk8u1pUGy3md1M9Co",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}